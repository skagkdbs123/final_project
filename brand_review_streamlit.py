import streamlit as st
import pandas as pd
import numpy as np
import re
from tqdm import tqdm

import matplotlib.pyplot as plt
import plotly.graph_objects as go
import seaborn as sns
import koreanize_matplotlib
import plotly.express as px

import koreanize_matplotlib
#%config InlineBackend.figure_format = 'retina'

from krwordrank.hangle import normalize
from krwordrank.word import KRWordRank
from kiwipiepy import Kiwi

import requests
from PIL import Image
from bs4 import BeautifulSoup
#import cv2

st.set_page_config(
    page_title="brand review analysis",
    page_icon="ğŸ‘—",
    layout="wide",
)

st.markdown("# ğŸ‘• ë¸Œëœë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”. ğŸ‘–")

st.sidebar.markdown("# ë¸Œëœë“œ ì„ íƒ â“")

# select brand
brand_list = ['ë¸Œëœë“œ ì„ íƒ', 'ë¼í¼ì§€ìŠ¤í† ì–´', 'ê¼¼íŒŒë‡¨', 'ë“œë¡œìš°í•', 'ì¸ì‚¬ì¼ëŸ°ìŠ¤',
            'ì»¤ë²„ë‚«', 'íŒŒë¥´í‹°ë©˜í† ', 'í•„ë£¨ë¯¸ë„¤ì´íŠ¸', 'ì™€ë¦¿ì´ì¦Œ', 'ìˆ˜ì•„ë ˆ',
            'ë‚´ì…”ë„ì§€ì˜¤ê·¸ë˜í”½', 'ì˜ˆì¼', 'ë””ì¦ˆì´ìŠ¤ë„¤ë²„ëŒ“', 'ì•„ì›ƒìŠ¤íƒ ë”©', 'ë¦¬',
            'ì–´ë°˜ë“œë ˆìŠ¤']
choice = st.selectbox('ğŸ”ë³´ê³ ì í•˜ëŠ” ë¸Œëœë“œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”.', brand_list)

select_brand = []
for num in range(len(brand_list)):
    if num == 0:
        pass
    elif choice == brand_list[num]:
        st.write(f'{brand_list[num]}ë¥¼ ì„ íƒí•˜ì…¨êµ°ìš”. â³ ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”.')
        select_brand.append(f'{brand_list[num]}')
    

# data load
brand_link = {
    'ë¼í¼ì§€ìŠ¤í† ì–´' : '1fr6RZGM_vd5L0IDS0XIJCcScn_QtFKYz',
    'ë“œë¡œìš°í•' : '1KoWlv8cQXI9kpmfVL1pSr6jLH1RvkXkt',
    'ì»¤ë²„ë‚«' : '1we5q5975vDb2iNrmTPfDNsCNrQxrrWTQ',
    'íŒŒë¥´í‹°ë©˜í† ' : '1D1BhygdkEvZU4uQQ1Y450zpZVHbdDiwc',
    'í•„ë£¨ë¯¸ë„¤ì´íŠ¸' : '190VQjL5F-8KPxQlYj3_8pY9Fb9JzmPIi',
    'ê¼¼íŒŒë‡¨' : '1AjkzWni2Lp1V2vzhe3-gwzjDT1ASCsA2',
    'ì¸ì‚¬ì¼ëŸ°ìŠ¤' : '12vrFmoKeJ_UHaXljvsO1l4rbvV1Tw4Dq' ,
    'ì™€ë¦¿ì´ì¦Œ' : '1C_hPRBxb0sp6bJdVV4OEmNmMWFqYILQZ' ,
    'ìˆ˜ì•„ë ˆ' : '1KZSMUTjqqGGMVOp5KiH7X_n23IcJL3wB',
    'ë‚´ì…”ë„ì§€ì˜¤ê·¸ë˜í”½' : '1gWhGfIluszy8oVO-RwZIKtxJ0RM8yMhn',
    'ì˜ˆì¼' : '1eIoxQZrDLhsCWEl1-NJ9yE3Biem3VIcG',    
    'ë””ì¦ˆì´ìŠ¤ë„¤ë²„ëŒ“' : '1ZgW4xBoXcNczxjMdw6YMKVEdjtgsUoyK',    
    'ì•„ì›ƒìŠ¤íƒ ë”©' : '1KfcwqNRRbKLgCNvgMIgZiRLaJ8Si8BJU',
    'ë¦¬' : '1fqw2TiNEDxyrkaSDIlcxQ6UUyUD38kgW',
    'ì–´ë°˜ë“œë ˆìŠ¤' : '1YmNK_XSR03fcKgnt6ZOmWkAusXteV8tT' 
}

def data_load(select_brand):
    data_link = 'https://drive.google.com/uc?id='+brand_link[select_brand]
    data = pd.read_csv(data_link) 
    return data

  
try : 
    data_load_state = st.text('Loading data...') 
    data = data_load(select_brand[0])
except KeyError as k:
    pass
except IndexError as i:
    pass

# labeling
def labeling(data):
    df = data[["ë¦¬ë·°", "í‰ì "]]
    df = df.reset_index(drop=True)

    ì‚­ì œ = df[(df["í‰ì "]=="60%") | (df["í‰ì "]=="80%")].index
    df = df.drop(ì‚­ì œ)

    df.loc[(df["í‰ì "] == "100%"), "label"] = 1
    df.loc[(df["í‰ì "] == "20%") | (df["í‰ì "] == "40%"), "label"] = 0

    df = df.drop_duplicates()
    df = df.reset_index(drop=True)
    return df

# ë¬¸ì ì „ì²˜ë¦¬
def preprocessing(text):
    text = re.sub('[^ê°€-í£ã„±-ã…ã…-ã…£a-zA]', " ", text)
    text = re.sub('[\s]+', " ", text)
    text = text.lower()
    return text

try :
    label_data = labeling(data=data)
    positive = label_data[(label_data["í‰ì "] == "100%")]
    negative = label_data[(label_data["í‰ì "] == "20%") | (label_data["í‰ì "] == "40%")]
    positive['ë¦¬ë·°'] = positive['ë¦¬ë·°'].map(preprocessing)
    negative['ë¦¬ë·°'] = negative['ë¦¬ë·°'].map(preprocessing)
    data_load_state.text(f'{select_brand[0]} ë°ì´í„° ë¡œë“œ success â€¼')
except KeyError as k:
    pass
except NameError as n:
    pass

# Kiwi ì ìš©
def kiwi(sentence):
    results = []
    result = Kiwi().analyze(sentence)
    for token, pos, _, _ in result[0][0]:
        if len(token) != 1 and pos.startswith('N') or pos.startswith('SL'):
            results.append(token)
    return results

try :
    pos_noun_list = []
    neg_noun_list = []
    for pos in positive['ë¦¬ë·°'].tolist():
        pos_nouns = kiwi(pos)
        pos_text = ' '.join(pos_nouns)
        pos_noun_list.append(pos_text)

    for neg in negative['ë¦¬ë·°'].tolist():
        neg_nouns = kiwi(neg)
        neg_text = ' '.join(neg_nouns)
        neg_noun_list.append(neg_text)

except KeyError as k:
    pass
except NameError as n:
    pass

# wordrank ì ìš© top10 í‚¤ì›Œë“œ ë½‘ê¸°
def word_rank(corpus):
    beta = 0.90    # PageRankì˜ decaying factor beta
    max_iter = 5
    top_keywords = []
    fnames = [corpus]
    top_10=[]
    
    for fname in fnames:
        texts = fname
        wordrank_extractor = KRWordRank(min_count=5, max_length=10, verbose=False)
        keywords, rank, graph = wordrank_extractor.extract(texts, beta, max_iter)
        top_keywords.append(sorted(keywords.items(),key=lambda x:x[1],reverse=True)[:100])
        
    for i in range(10):
        if i<10:
            top_10.append(top_keywords[0][i][0])
            i += 1
    return top_10

# img data load
img_brand_link = {
    'ë¼í¼ì§€ìŠ¤í† ì–´_img' : '1QfwKjzDAfoowpe4LiH9yxhChdZo3iizh',
    'ë“œë¡œìš°í•_img' : '1-caqKnBlM4Q26tec_4aWFm9017F9-_E4',
    'ì»¤ë²„ë‚«_img' : '1f50QDJI6K7KeZ7WGSANV1sH6aiXa2c5T',
    'íŒŒë¥´í‹°ë©˜í† _img' : '1Nt0LAAWlvVTh60Y9Zvbb3jmw0Jdl-URg',
    'í•„ë£¨ë¯¸ë„¤ì´íŠ¸_img' : '1CtYGt4E5hzqp-tvwix5WhANpvzds8TQK',
    'ê¼¼íŒŒë‡¨_img' : '1-CJcWAp3WxKymk7PUxj9NYgM-3zfjM83',
    'ì¸ì‚¬ì¼ëŸ°ìŠ¤_img' : '1COUpes3WPXeGn6mdYrtzG1D9dMJ43SF7' ,
    'ì™€ë¦¿ì´ì¦Œ_img' : '1nxOa5_69KQrmbMduDhQtAPFgiclIFXO_' ,
    'ìˆ˜ì•„ë ˆ_img' : '1G5QtrNYtKNhFgRVx0Fj2-b0F626o7ccX',
    'ë‚´ì…”ë„ì§€ì˜¤ê·¸ë˜í”½_img' : '1Wm2ox9koFbYXkMtvLApCfQjDnPfSRsGF',
    'ì˜ˆì¼_img' : '1MxqLCSCptl5O5shldxK513mh4G7z3j3V',    
    'ë””ì¦ˆì´ìŠ¤ë„¤ë²„ëŒ“_img' : '17yuM4U3W3aKKMCQphvBePiHC5mVugVkf',    
    'ì•„ì›ƒìŠ¤íƒ ë”©_img' : '17v-GwoTF0mgOkRta3hAhytWLPOh2YUpk',
    'ë¦¬_img' : '1us6tb40vHoz4hrNGfoD9n2BL0fciY97n',
    'ì–´ë°˜ë“œë ˆìŠ¤_img' : '1LDAyqzM-TZZCLVrZJK08WLJe2IF6Jtxt' 
}

def img_data_load(select_brand):
    img_data_link = 'https://drive.google.com/uc?id='+img_brand_link[select_brand]
    img_data = pd.read_csv(img_data_link) 
    return img_data    


def keyword_review(link_csv, data, keywords):
    img_csv = pd.read_csv('https://drive.google.com/uc?id='+brand_link[select_brand])

    for keyword_count in range(len(keywords)):
        if st.button(keywords[keyword_count]):
            keyword_review_data = data[data['ë¦¬ë·°'].str.contains(f'{keywords[keyword_count]}')]

            product_num = keyword_review['ìƒí’ˆ_num']
            top3_cumc_product = product_num.value_counts().sort_values(ascending=False)[:3].index

            review1 = keyword_review_data[keyword_review_data['ìƒí’ˆ_num']==f'{top3_cumc_product[0]}'].sample(3)
            review2 = keyword_review_data[keyword_review_data['ìƒí’ˆ_num']==f'{top3_cumc_product[1]}'].sample(3)
            review3 = keyword_review_data[keyword_review_data['ìƒí’ˆ_num']==f'{top3_cumc_product[2]}'].sample(3)

            img_link = []
            for num in product_num:
                link = link_csv[link_csv['ìƒí’ˆ']==num]
                img_link.append(link['ì‚¬ì§„'])

            # Load the image from the URL
            for i in range(len(img_link)):
                URL = f'https:{img_link}'
                response = requests.get(URL)
                image = Image.open(BytesIO(response.content))

                st.image(image, caption='Image from URL')
                st.text(f'review{i}')
        

try :
    pos_keyword = word_rank(pos_noun_list)
    neg_keyword = word_rank(neg_noun_list)

    img_link = img_data_load(select_brand)
    keyword_review(img_link, positive, pos_keyword)
    keyword_review(img_link, positive, neg_keyword)
except KeyError as k:
    pass
except NameError as n:
    pass

